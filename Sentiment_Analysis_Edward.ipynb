{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Edward.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIfv6tav4qScHLpSw4RfBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edward2018211/sentiment-analysis-SOS/blob/master/Sentiment_Analysis_Edward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1al0gyn83YM",
        "colab_type": "text"
      },
      "source": [
        "This notebook is a part of the News Sentiment Analysis project and contains snippets of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_9Su4QI9UFr",
        "colab_type": "text"
      },
      "source": [
        "We will first scrape images from Google using a simple script for processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzXvdI628lwO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "526e3de8-1d00-4255-dca2-76420d7b0088"
      },
      "source": [
        "print(\"Hello World!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbeWrZkA-FVU",
        "colab_type": "text"
      },
      "source": [
        "Next, we will manually label the images that are scraped to use as training data for our model in Tensorflow. An additional suggestion is that we could use data augmentation for more data, but we do need be aware of photos that wouldn't make sense to data augment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0_sayLC-WbK",
        "colab_type": "text"
      },
      "source": [
        "We will need to do some feature engineering for better prediction accuracy. We'll need multiple layers for our deep neural network and we'll probably want to work in increased stride length for a faster model that uses less memory. Probably dropout too to combat overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNNZJx7M-fIb",
        "colab_type": "text"
      },
      "source": [
        "Once the model is fine-tuned, it will be able to make predictions based on new images (data)."
      ]
    }
  ]
}