{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Edward.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMz93hwdbJ79hXmFmGLAcgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edward2018211/sentiment-analysis-SOS/blob/master/Sentiment_Analysis_Edward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1al0gyn83YM",
        "colab_type": "text"
      },
      "source": [
        "This notebook is a part of the News Sentiment Analysis project and contains snippets of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_9Su4QI9UFr",
        "colab_type": "text"
      },
      "source": [
        "We will first scrape images from Google using a simple script for processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbeWrZkA-FVU",
        "colab_type": "text"
      },
      "source": [
        "Next, we will manually label the images that are scraped to use as training data for our model in Tensorflow. An additional suggestion is that we could use data augmentation for more data, but we do need be aware of photos that wouldn't make sense to data augment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0_sayLC-WbK",
        "colab_type": "text"
      },
      "source": [
        "We will need to do some feature engineering for better prediction accuracy. We'll need multiple layers for our deep neural network and we'll probably want to work in increased stride length for a faster model that uses less memory. Probably dropout too to combat overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNNZJx7M-fIb",
        "colab_type": "text"
      },
      "source": [
        "Once the model is fine-tuned, it will be able to make predictions based on new images (data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvHhYyKabUFy",
        "colab_type": "text"
      },
      "source": [
        "In addition, we would also like to analyze text associated with the news and not simply pictures. The classical approach is the bag of words approach, instead we'll use word vectors which enhances performance to train our SVM. Below is the setup for our text model. \n",
        "\n",
        "Note: The dataset contains around 1.6 million texts, so loading the data and training the model takes 20 - 30 minutes when this project is completed. Currently we are able to extract information from all 1.6 million training points.\n",
        "\n",
        "Next Steps:\n",
        "1. We want to experiment with other ML models and how accurate they are on the training and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5dRaVDcbhPI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbd68b56-33bf-4eb3-f236-5bb283f8f38b"
      },
      "source": [
        "# Download large model\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en\n",
        "\n",
        "# Download Google API package\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade google-api-python-client\n",
        "\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "import io\n",
        "import csv\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Need to load the large model to get the vectors\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=13324076c97eacdfcabdc58cf460896e5905f45f950b6c3571bfd6b08ced39db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f4z3_fqv/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.1.1\n",
            "Collecting google-api-python-client\n",
            "  Downloading google_api_python_client-1.10.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.17.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.17.2)\n",
            "Collecting google-api-core<2dev,>=1.18.0\n",
            "  Downloading google_api_core-1.22.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.4)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.16.0->google-api-python-client) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (1.24.3)\n",
            "\u001b[31mERROR: google-api-core 1.22.0 has requirement google-auth<2.0dev,>=1.19.1, but you'll have google-auth 1.17.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-api-core, google-api-python-client\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.16.0\n",
            "    Uninstalling google-api-core-1.16.0:\n",
            "      Successfully uninstalled google-api-core-1.16.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed google-api-core-1.22.0 google-api-python-client-1.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5Sdv0FtKS7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVHNyPvP2srk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the sentiment data\n",
        "downloaded = drive.CreateFile({'id': '1YcCIUOA0-5lNI3xKZvi33Vzg3X0lABUh'})\n",
        "downloaded.GetContentFile('training.1600000.processed.noemoticon.csv') \n",
        "sentiment = pd.read_csv('training.1600000.processed.noemoticon.csv', names=['polarity', 'id', 'date', 'query', 'user', 'text'], encoding=\"latin-1\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tge0ZIJGn5xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle data set\n",
        "sentiment = sentiment.sample(frac=1)\n",
        "\n",
        "# Get first 5000 rows\n",
        "sentimentHead = sentiment\n",
        "#sentiment.head(50000)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGSsrJhQwphk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a19af09a-b69e-470d-afc1-c1145f655145"
      },
      "source": [
        "# Dictionary of all emojis mapping to their meanings.\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "\n",
        "## Set of all stopwords in english.\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
        "\n",
        "# Preprocess data\n",
        "processedText = []\n",
        "    \n",
        "# Create Lemmatizer and Stemmer\n",
        "wordLemm = WordNetLemmatizer()\n",
        "    \n",
        "# Defining regex patterns.\n",
        "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "userPattern       = '@[^\\s]+'\n",
        "alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "sequencePattern   = r\"(.)\\1\\1+\"\n",
        "seqReplacePattern = r\"\\1\\1\"\n",
        "    \n",
        "for tweet in sentimentHead['text']:\n",
        "  # Make all tweets lowercase\n",
        "  tweet = tweet.lower()\n",
        "        \n",
        "  # Replace all URLs with 'URL'\n",
        "  tweet = re.sub(urlPattern,' URL',tweet)\n",
        "\n",
        "  # Replace all emojis\n",
        "  for emoji in emojis.keys():\n",
        "    tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
        "    \n",
        "  # Replace @USERNAME to 'USER'\n",
        "  tweet = re.sub(userPattern,' USER', tweet)        \n",
        "    \n",
        "  # Replace all non alphabets\n",
        "  tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "    \n",
        "  # Replace 3 or more consecutive letters by 2 letter\n",
        "  tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "\n",
        "  tweetwords = ''\n",
        "  for word in tweet.split():\n",
        "    # Checking if the word is a stopword\n",
        "    # if word not in stopwordlist:\n",
        "    if len(word) > 1:\n",
        "      # Lemmatize word\n",
        "      word = wordLemm.lemmatize(word)\n",
        "      tweetwords += (word + ' ')\n",
        "            \n",
        "  processedText.append(tweetwords)\n",
        "\n",
        "print(\"Preprocessed data\")\n",
        "\n",
        "# Add preprocessed data to arrays for input to SVM\n",
        "with nlp.disable_pipes():\n",
        "  sentiment_binary = np.array([row for row in sentimentHead['polarity']])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(processedText, sentiment_binary,\n",
        "                                                    test_size=0.03, random_state=0)\n",
        "\n",
        "# Vectorize data\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "vectoriser.fit(X_train)\n",
        "clearXTrain = X_train\n",
        "\n",
        "X_train = vectoriser.transform(X_train)\n",
        "X_test  = vectoriser.transform(X_test)\n",
        "\n",
        "print(\"Data vectorized\")\n",
        "\n",
        "# Set dual=False to speed up training, and it's not needed\n",
        "svc = LinearSVC(random_state=0, dual=False, max_iter=10000)\n",
        "svc.fit(X_train, y_train)\n",
        "print(\"Fitted data\")\n",
        "score = svc.score(X_test, y_test)\n",
        "print(f\"Accuracy: {score * 100:.3f}%\", )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessed data\n",
            "Data vectorized\n",
            "Fitted data\n",
            "Accuracy: 81.550%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Vdb7yVNfG0",
        "colab_type": "text"
      },
      "source": [
        "The next step in the process will be to make predictions on unseen data. The code below does that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyfRPLc1Nd99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "tuple_objects = (svc, clearXTrain, y_train, score)\n",
        "\n",
        "# Save tuple\n",
        "pickle.dump(tuple_objects, open(\"text_model.pkl\", 'wb'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc3BnPINCyIo",
        "colab_type": "text"
      },
      "source": [
        "We can separately also use Google Cloud to analyze sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axkXXVc_CxcD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "outputId": "f3f68f4b-2d38-43ab-b786-a229b0d67868"
      },
      "source": [
        "from google.cloud import language_v1\n",
        "from google.cloud.language_v1 import enums\n",
        "from googleapiclient.discovery import build\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set Google credentials\n",
        "# Note: Make sure to upload the json file into Colab\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/news-sentiment-analysis-283320-5af01bb429bf.json\"\n",
        "\n",
        "\n",
        "def analyze_sentiment(text_content):\n",
        "    \"\"\"\n",
        "    Analyzing Sentiment in a String\n",
        "    Args:\n",
        "      text_content The text content to analyze\n",
        "    \"\"\"\n",
        "\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "\n",
        "    # text_content = 'I am so happy and joyful.'\n",
        "\n",
        "    # Available types: PLAIN_TEXT, HTML\n",
        "    type_ = enums.Document.Type.PLAIN_TEXT\n",
        "\n",
        "    # Optional. If not specified, the language is automatically detected.\n",
        "    # For list of supported languages:\n",
        "    # https://cloud.google.com/natural-language/docs/languages\n",
        "    language = \"en\"\n",
        "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
        "\n",
        "    # Available values: NONE, UTF8, UTF16, UTF32\n",
        "    encoding_type = enums.EncodingType.UTF8\n",
        "\n",
        "    response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
        "    # Get overall sentiment of the input document\n",
        "    print(u\"Document sentiment score: {}\".format(response.document_sentiment.score))\n",
        "    print(\n",
        "        u\"Document sentiment magnitude: {}\".format(\n",
        "            response.document_sentiment.magnitude\n",
        "        )\n",
        "    )\n",
        "    # Get sentiment for all sentences in the document\n",
        "    for sentence in response.sentences:\n",
        "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
        "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
        "        print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
        "\n",
        "    # Get the language of the text, which will be the same as\n",
        "    # the language specified in the request or, if not specified,\n",
        "    # the automatically-detected language.\n",
        "    print(u\"Language of the text: {}\".format(response.language))\n",
        "\n",
        "APIKEY = getpass.getpass()\n",
        "\n",
        "lservice = build('language', 'v1beta1', developerKey=APIKEY)\n",
        "sentimentEmotion = []\n",
        "shortText = processedText[600:]\n",
        "\n",
        "for quote in shortText:\n",
        "  response = lservice.documents().analyzeSentiment(\n",
        "    body={\n",
        "      'document': {\n",
        "         'type': 'PLAIN_TEXT',\n",
        "         'content': quote\n",
        "      }\n",
        "    }).execute()\n",
        "  polarity = 1 if response['documentSentiment']['polarity'] == 1 else 0\n",
        "  sentimentEmotion.append(polarity)\n",
        "  #magnitude = response['documentSentiment']['magnitude']\n",
        "  #print('POLARITY=%s MAGNITUDE=%s for %s' % (polarity, magnitude, quote))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(shortText, sentimentEmotion,\n",
        "                                                    test_size=0.03, random_state=0)\n",
        "\n",
        "# Vectorize data\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "vectoriser.fit(X_train)\n",
        "\n",
        "X_train = vectoriser.transform(X_train)\n",
        "X_test  = vectoriser.transform(X_test)\n",
        "\n",
        "print(\"Data vectorized\")\n",
        "\n",
        "# Set dual=False to speed up training, and it's not needed\n",
        "svc = LinearSVC(random_state=0, dual=False, max_iter=10000)\n",
        "svc.fit(X_train, y_train)\n",
        "print(\"Fitted data\")\n",
        "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    try:\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 42, in autodetect\n",
            "    pass\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-51cfaee87520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m       'document': {\n\u001b[1;32m     64\u001b[0m          \u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'PLAIN_TEXT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m          \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m       }\n\u001b[1;32m     67\u001b[0m     }).execute()\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sleep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 429 when requesting https://language.googleapis.com/v1beta1/documents:analyzeSentiment?key=AIzaSyCQ-TSXDUm-ENbpxWOGr_IL_1ovCWVblRs&alt=json returned \"Quota exceeded for quota metric 'Requests' and limit 'Requests per minute' of service 'language.googleapis.com' for consumer 'project_number:776911207756'.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Google developer console API key', 'url': 'https://console.developers.google.com/project/776911207756/apiui/credential'}]}]\">"
          ]
        }
      ]
    }
  ]
}